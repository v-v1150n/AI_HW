
Using cuda device
==((====))==  Unsloth: Fast Mistral patching release 2024.5
   \\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.528 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:332: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.
  warnings.warn(






























Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12730/12730 [01:01<00:00, 207.14 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:00<00:00, 188.67 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 12,730 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 2 | Gradient Accumulation steps = 8
\        /    Total batch size = 16 | Total steps = 795
 "-____-"     Number of trainable parameters = 41,943,040
[34m[1mwandb[39m[22m: [33mWARNING[39m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.

  0%|                                                                                                                                                                          | 0/795 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed
  0%|â–                                                                                                                                                               | 1/795 [00:15<3:26:23, 15.60s/it]

  0%|â–                                                                                                                                                               | 2/795 [00:30<3:18:23, 15.01s/it]

  0%|â–Œ                                                                                                                                                               | 3/795 [00:43<3:10:06, 14.40s/it]

  1%|â–Š                                                                                                                                                               | 4/795 [00:58<3:11:49, 14.55s/it]

  1%|â–ˆ                                                                                                                                                               | 5/795 [01:11<3:04:25, 14.01s/it]

  1%|â–ˆâ–                                                                                                                                                              | 6/795 [01:24<3:01:02, 13.77s/it]

  1%|â–ˆâ–                                                                                                                                                              | 7/795 [01:40<3:07:38, 14.29s/it]

  1%|â–ˆâ–Œ                                                                                                                                                              | 8/795 [01:53<3:03:35, 14.00s/it]

  1%|â–ˆâ–Š                                                                                                                                                              | 9/795 [02:05<2:54:22, 13.31s/it]
{'loss': 0.6329, 'learning_rate': 4.998419062351724e-06, 'rewards/chosen': -0.04028179869055748, 'rewards/rejected': -0.16617344319820404, 'rewards/accuracies': 0.9375, 'rewards/margins': 0.12589162588119507, 'logps/rejected': -207.74620056152344, 'logps/chosen': -180.13552856445312, 'logits/rejected': -3.316779136657715, 'logits/chosen': -2.9655466079711914, 'epoch': 0.01}


  1%|â–ˆâ–ˆâ–                                                                                                                                                            | 11/795 [02:30<2:48:30, 12.90s/it]

  2%|â–ˆâ–ˆâ–                                                                                                                                                            | 12/795 [02:42<2:44:03, 12.57s/it]

  2%|â–ˆâ–ˆâ–Œ                                                                                                                                                            | 13/795 [02:55<2:42:51, 12.50s/it]

  2%|â–ˆâ–ˆâ–Š                                                                                                                                                            | 14/795 [03:07<2:44:00, 12.60s/it]
{'loss': 0.5935, 'learning_rate': 4.996175093713907e-06, 'rewards/chosen': -0.09627380967140198, 'rewards/rejected': -0.3102684020996094, 'rewards/accuracies': 1.0, 'rewards/margins': 0.2139946073293686, 'logps/rejected': -272.04669189453125, 'logps/chosen': -193.68405151367188, 'logits/rejected': -3.3927717208862305, 'logits/chosen': -2.672286033630371, 'epoch': 0.02}


  2%|â–ˆâ–ˆâ–ˆâ–                                                                                                                                                           | 16/795 [03:38<3:01:54, 14.01s/it]

  2%|â–ˆâ–ˆâ–ˆâ–                                                                                                                                                           | 17/795 [03:51<2:57:56, 13.72s/it]

  2%|â–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                                           | 18/795 [04:06<3:00:14, 13.92s/it]

  2%|â–ˆâ–ˆâ–ˆâ–Š                                                                                                                                                           | 19/795 [04:21<3:04:32, 14.27s/it]

  3%|â–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                                           | 20/795 [04:36<3:07:47, 14.54s/it]

  3%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                          | 21/795 [04:49<3:03:26, 14.22s/it]

  3%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                          | 22/795 [05:03<3:02:30, 14.17s/it]

  3%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                                          | 23/795 [05:21<3:14:08, 15.09s/it]
{'loss': 0.5301, 'learning_rate': 4.989681128960714e-06, 'rewards/chosen': -0.2916850745677948, 'rewards/rejected': -0.7005159258842468, 'rewards/accuracies': 0.875, 'rewards/margins': 0.4088308811187744, 'logps/rejected': -221.15780639648438, 'logps/chosen': -192.2086639404297, 'logits/rejected': -3.315913200378418, 'logits/chosen': -3.1768908500671387, 'epoch': 0.03}


  3%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                                          | 25/795 [05:47<3:01:01, 14.11s/it]

  3%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                         | 26/795 [06:01<3:01:13, 14.14s/it]

  3%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                         | 27/795 [06:14<2:57:59, 13.91s/it]

  4%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                                         | 28/795 [06:30<3:04:15, 14.41s/it]

  4%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                                                         | 29/795 [06:45<3:05:14, 14.51s/it]

  4%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                                         | 30/795 [07:00<3:08:27, 14.78s/it]

  4%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                        | 31/795 [07:18<3:18:27, 15.59s/it]
{'loss': 0.405, 'learning_rate': 4.98126491336793e-06, 'rewards/chosen': -0.40579524636268616, 'rewards/rejected': -1.2885624170303345, 'rewards/accuracies': 0.9375, 'rewards/margins': 0.8827670812606812, 'logps/rejected': -227.20372009277344, 'logps/chosen': -317.35357666015625, 'logits/rejected': -3.292423725128174, 'logits/chosen': -3.2838101387023926, 'epoch': 0.04}


  4%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                                        | 33/795 [07:45<3:05:27, 14.60s/it]

  4%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                                                        | 34/795 [07:59<3:03:12, 14.44s/it]

  4%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                                                                        | 35/795 [08:13<3:00:34, 14.26s/it]

  5%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                       | 36/795 [08:27<2:58:24, 14.10s/it]

  5%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                       | 37/795 [08:43<3:05:43, 14.70s/it]

  5%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                                       | 38/795 [08:55<2:56:54, 14.02s/it]

  5%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                                                       | 39/795 [09:12<3:05:57, 14.76s/it]

  5%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                                       | 40/795 [09:25<2:58:59, 14.22s/it]
{'loss': 0.296, 'learning_rate': 4.96883328387375e-06, 'rewards/chosen': -0.3683076500892639, 'rewards/rejected': -1.8102065324783325, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4418989419937134, 'logps/rejected': -179.25804138183594, 'logps/chosen': -183.64523315429688, 'logits/rejected': -3.3176257610321045, 'logits/chosen': -2.9913089275360107, 'epoch': 0.05}


  5%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                      | 42/795 [09:51<2:51:33, 13.67s/it]

  5%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                                      | 43/795 [10:06<2:56:07, 14.05s/it]

  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                                                      | 44/795 [10:21<3:00:09, 14.39s/it]

  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                                      | 45/795 [10:35<2:59:33, 14.36s/it]

  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                     | 46/795 [10:51<3:04:12, 14.76s/it]

  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                     | 47/795 [11:03<2:55:42, 14.09s/it]

  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                                     | 48/795 [11:16<2:48:49, 13.56s/it]

  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                                                     | 49/795 [11:31<2:54:01, 14.00s/it]

  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                                     | 50/795 [11:44<2:50:31, 13.73s/it]

  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                    | 51/795 [11:58<2:50:49, 13.78s/it]

  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                    | 52/795 [12:13<2:54:38, 14.10s/it]

  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                                    | 53/795 [12:27<2:55:35, 14.20s/it]

  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                                                    | 54/795 [12:42<2:58:29, 14.45s/it]

  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                                    | 55/795 [12:58<3:04:19, 14.94s/it]

  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                   | 56/795 [13:11<2:55:09, 14.22s/it]

  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                   | 57/795 [13:26<2:57:07, 14.40s/it]

  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                                   | 58/795 [13:42<3:02:55, 14.89s/it]

  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                                                   | 59/795 [13:55<2:55:30, 14.31s/it]

  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                                   | 60/795 [14:11<3:01:55, 14.85s/it]

  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                  | 61/795 [14:22<2:49:54, 13.89s/it]
{'loss': 0.1425, 'learning_rate': 4.927717826132723e-06, 'rewards/chosen': -0.7382562160491943, 'rewards/rejected': -3.1720592975616455, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4338033199310303, 'logps/rejected': -259.5151062011719, 'logps/chosen': -302.3341979980469, 'logits/rejected': -3.1577305793762207, 'logits/chosen': -3.135037899017334, 'epoch': 0.08}


  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                                  | 63/795 [14:51<2:53:51, 14.25s/it]

  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                                                  | 64/795 [15:05<2:50:32, 14.00s/it]

  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                                  | 65/795 [15:17<2:45:08, 13.57s/it]

  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                 | 66/795 [15:35<3:02:00, 14.98s/it]

  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                 | 67/795 [15:51<3:03:04, 15.09s/it]
  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                 | 67/795 [15:51<3:03:04, 15.09s/it]Traceback (most recent call last):
  File "/home/vv1150n/python_file/AI2024-hw6/main.py", line 80, in <module>
    DPO.DPO_train(args, output_dir)
  File "/home/vv1150n/python_file/AI2024-hw6/DPO.py", line 105, in DPO_train
    dpo_trainer.train()
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/transformers/trainer.py", line 1885, in train
    return inner_training_loop(
  File "<string>", line 348, in _fast_inner_training_loop
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/transformers/trainer.py", line 3238, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py", line 1081, in compute_loss
    loss, metrics = self.get_batch_loss_metrics(model, inputs, train_eval="train")
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py", line 1022, in get_batch_loss_metrics
    ) = self.concatenated_forward(model, batch)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py", line 985, in concatenated_forward
    all_logits = model(
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/accelerate/utils/operations.py", line 822, in forward
    return model_forward(*args, **kwargs)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/accelerate/utils/operations.py", line 810, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast
    return func(*args, **kwargs)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/unsloth/models/llama.py", line 883, in PeftModelForCausalLM_fast_forward
    return self.base_model(
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 179, in forward
    return self.model.forward(*args, **kwargs)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/unsloth/models/mistral.py", line 213, in MistralForCausalLM_fast_forward
    outputs = self.model(
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/unsloth/models/llama.py", line 669, in LlamaModel_fast_forward
    layer_outputs = torch.utils.checkpoint.checkpoint(
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/torch/_compile.py", line 24, in inner
    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 451, in _fn
    return fn(*args, **kwargs)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 487, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/unsloth/models/llama.py", line 665, in custom_forward
    return module(*inputs, past_key_value, output_attentions, padding_mask = padding_mask)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/unsloth/models/llama.py", line 449, in LlamaDecoderLayer_fast_forward
    hidden_states = self.mlp(hidden_states)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/unsloth/kernels/fast_lora.py", line 152, in apply_lora_mlp_swiglu
    out = LoRA_MLP.apply(X,
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/torch/autograd/function.py", line 598, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/torch/cuda/amp/autocast_mode.py", line 115, in decorate_fwd
    return fwd(*args, **kwargs)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/unsloth/kernels/fast_lora.py", line 69, in forward
    i = matmul_lora(h, downW, downW_quant, downA, downB, downS)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/unsloth/kernels/utils.py", line 235, in matmul_lora
    out = torch.matmul(X, W, out = out)
KeyboardInterrupt