
Using cuda device
`low_cpu_mem_usage` was None, now set to True since model is quantized.
























































model.safetensors: 100%|███████████████████| 5.70G/5.70G [01:51<00:00, 51.0MB/s]
generation_config.json: 100%|███████████████████| 172/172 [00:00<00:00, 363kB/s]
Traceback (most recent call last):
  File "/home/vv1150n/python_file/AI2024-hw6/main.py", line 82, in <module>
    ORPO.ORPO_train(args, output_dir)
  File "/home/vv1150n/python_file/AI2024-hw6/ORPO.py", line 29, in ORPO_train
    model = AutoModelForCausalLM.from_pretrained(args.model_name, torch_dtype=torch_dtype).to(device)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/accelerate/big_modeling.py", line 456, in wrapper
    return fn(*args, **kwargs)
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2702, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.