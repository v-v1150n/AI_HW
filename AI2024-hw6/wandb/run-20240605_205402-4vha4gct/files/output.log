
Using cuda device
==((====))==  Unsloth: Fast Llama patching release 2024.5
   \\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.528 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
tokenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50.6k/50.6k [00:00<00:00, 277kB/s]
tokenizer.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9.09M/9.09M [00:01<00:00, 7.38MB/s]
special_tokens_map.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 464/464 [00:00<00:00, 1.08MB/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Traceback (most recent call last):
  File "/home/vv1150n/python_file/AI2024-hw6/main.py", line 80, in <module>
    DPO.DPO_train(args, output_dir)
  File "/home/vv1150n/python_file/AI2024-hw6/DPO.py", line 177, in DPO_train
    model = FastLanguageModel.get_peft_model(model, "lora_weights")
  File "/home/vv1150n/miniconda3/envs/ai_hw6/lib/python3.10/site-packages/unsloth/models/llama.py", line 1424, in get_peft_model
    assert(max_seq_length <= model.max_seq_length)
AttributeError: 'tuple' object has no attribute 'max_seq_length'